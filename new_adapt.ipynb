{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be406c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting IAT stats per device (optimized)...\n",
      "Collecting IAT stats from 50 sample files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling IATs: 100%|██████████| 50/50 [00:13<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing adaptive thresholds per device...\n",
      "Processing captures with adaptive thresholds...\n",
      "Processing 1760 files with 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/18: 100%|██████████| 100/100 [00:04<00:00, 22.43it/s]\n",
      "Processing chunk 2/18: 100%|██████████| 100/100 [00:03<00:00, 32.26it/s]\n",
      "Processing chunk 3/18: 100%|██████████| 100/100 [00:03<00:00, 28.53it/s]\n",
      "Processing chunk 4/18: 100%|██████████| 100/100 [00:03<00:00, 25.05it/s]\n",
      "Processing chunk 5/18: 100%|██████████| 100/100 [00:06<00:00, 16.19it/s]\n",
      "Processing chunk 6/18: 100%|██████████| 100/100 [00:06<00:00, 15.52it/s]\n",
      "Processing chunk 7/18: 100%|██████████| 100/100 [00:07<00:00, 13.60it/s]\n",
      "Processing chunk 8/18: 100%|██████████| 100/100 [00:03<00:00, 27.13it/s]\n",
      "Processing chunk 9/18: 100%|██████████| 100/100 [00:03<00:00, 30.58it/s]\n",
      "Processing chunk 10/18: 100%|██████████| 100/100 [00:06<00:00, 14.37it/s]\n",
      "Processing chunk 11/18: 100%|██████████| 100/100 [00:07<00:00, 14.06it/s]\n",
      "Processing chunk 12/18: 100%|██████████| 100/100 [00:03<00:00, 32.54it/s]\n",
      "Processing chunk 13/18: 100%|██████████| 100/100 [00:01<00:00, 50.94it/s]\n",
      "Processing chunk 14/18: 100%|██████████| 100/100 [00:01<00:00, 70.05it/s]\n",
      "Processing chunk 15/18: 100%|██████████| 100/100 [00:01<00:00, 87.97it/s]\n",
      "Processing chunk 16/18: 100%|██████████| 100/100 [00:03<00:00, 29.28it/s]\n",
      "Processing chunk 17/18: 100%|██████████| 100/100 [00:03<00:00, 27.20it/s]\n",
      "Processing chunk 18/18: 100%|██████████| 60/60 [00:04<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 82013 bursts to adaptive_thresold.csv\n",
      "Adaptive thresholds used per device:\n",
      " - plug: 0.0039 sec\n",
      " - plug1: 1.0000 sec\n",
      " - wall_socket: 0.0043 sec\n",
      " - tabel_lamp: 0.0010 sec\n",
      " - switch: 0.0046 sec\n",
      " - switch1: 0.0045 sec\n",
      " - air_purifier: 0.0123 sec\n",
      " - motion_sensor: 0.0030 sec\n",
      " - motion_sensor1: 1.0000 sec\n",
      " - door_sensor: 0.0039 sec\n",
      " - door_sensor1: 1.0000 sec\n",
      " - baby_cam: 0.0010 sec\n",
      " - camera: 0.0131 sec\n",
      " - power_strip: 0.0056 sec\n",
      " - non_iot_device: 0.0010 sec\n",
      " - non_iot_device1: 1.0000 sec\n",
      " - non_iot_device2: 1.0000 sec\n",
      " - non_iot_device3: 0.0028 sec\n",
      " - non_iot_device4: 1.0000 sec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scapy.all import rdpcap, Dot11\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import gc\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIGURATION\n",
    "# ==========================================================\n",
    "CAPTURE_ROOT = \"capturedfiles\"\n",
    "OUTPUT_CSV = \"adaptive_thresold.csv\"\n",
    "\n",
    "DEVICE_MACS = {\n",
    "    \"plug\": \"c0:f8:53:de:cf:2a\",\n",
    "    \"plug1\": \"c0:f8:53:df:18:ea\",\n",
    "    \"wall_socket\": \"d8:d6:68:06:6d:65\",\n",
    "    \"tabel_lamp\": \"3c:0b:59:8f:25:42\",\n",
    "    \"switch\": \"38:2c:e5:1d:02:fb\",\n",
    "    \"switch1\": \"38:2c:e5:1c:cf:6e\",\n",
    "    \"air_purifier\": \"50:ec:50:94:7b:a3\",\n",
    "    \"motion_sensor\": \"f8:17:2d:b6:38:de\",\n",
    "    \"motion_sensor1\": \"f8:17:2d:b4:3d:5a\",\n",
    "    \"door_sensor\": \"18:de:50:54:8e:e9\",\n",
    "    \"door_sensor1\": \"18:de:50:50:39:37\",\n",
    "    \"baby_cam\": \"78:8b:2a:9c:80:1e\",\n",
    "    \"camera\": \"5c:4e:ee:ce:f8:3b\",\n",
    "    \"power_strip\": \"fc:3c:d7:53:f6:79\",\n",
    "    \"non_iot_device\": \"4e:c2:19:38:75:dd\",\n",
    "    \"non_iot_device1\": \"0c:e4:a0:e5:05:03\",\n",
    "    \"non_iot_device2\": \"d8:f3:bc:67:1c:a5\",\n",
    "    \"non_iot_device3\": \"b4:0e:de:38:64:95\",\n",
    "    \"non_iot_device4\": \"66:35:9b:0c:1c:c6\", \n",
    "}\n",
    "\n",
    "BSSID = \"14:eb:b6:be:d7:1e\"\n",
    "\n",
    "# ==========================================================\n",
    "# OPTIMIZED FILE PROCESSING\n",
    "# ==========================================================\n",
    "def find_all_cap_files(root_dir):\n",
    "    \"\"\"Find all .cap files efficiently\"\"\"\n",
    "    cap_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            if f.endswith(\".cap\"):\n",
    "                cap_files.append(os.path.join(root, f))\n",
    "    return cap_files\n",
    "\n",
    "def extract_all_devices_packets(packets):\n",
    "    \"\"\"Extract packets for all devices in one pass\"\"\"\n",
    "    device_packets = {mac: [] for mac in DEVICE_MACS.values()}\n",
    "    \n",
    "    for pkt in packets:\n",
    "        if not pkt.haslayer(Dot11):\n",
    "            continue\n",
    "\n",
    "        dot11 = pkt[Dot11]\n",
    "        ptype = getattr(dot11, \"type\", None)\n",
    "        psub = getattr(dot11, \"subtype\", None)\n",
    "\n",
    "        # Skip noisy frames\n",
    "        if ptype == 2 and psub in [4, 12]:\n",
    "            continue\n",
    "\n",
    "        src = dot11.addr2\n",
    "        dst = dot11.addr1\n",
    "\n",
    "        if src is None or dst is None:\n",
    "            continue\n",
    "\n",
    "        # Check if packet involves any of our devices\n",
    "        if src in DEVICE_MACS.values() and dst == BSSID:\n",
    "            if src in device_packets:\n",
    "                device_packets[src].append((float(pkt.time), len(pkt), pkt))\n",
    "        elif src == BSSID and dst in DEVICE_MACS.values():\n",
    "            if dst in device_packets:\n",
    "                device_packets[dst].append((float(pkt.time), len(pkt), pkt))\n",
    "    \n",
    "    # Sort packets by time for each device\n",
    "    for mac in device_packets:\n",
    "        device_packets[mac].sort(key=lambda x: x[0])\n",
    "    \n",
    "    return device_packets\n",
    "\n",
    "def collect_device_iat_stats_optimized(root_dir, sample_size=50):\n",
    "    \"\"\"Collect IAT stats using sampling to avoid processing all files\"\"\"\n",
    "    cap_files = find_all_cap_files(root_dir)\n",
    "    device_iats = {dev: [] for dev in DEVICE_MACS.keys()}\n",
    "    \n",
    "    # Use only a sample of files for IAT calculation\n",
    "    if len(cap_files) > sample_size:\n",
    "        sample_files = np.random.choice(cap_files, sample_size, replace=False)\n",
    "    else:\n",
    "        sample_files = cap_files\n",
    "    \n",
    "    print(f\"Collecting IAT stats from {len(sample_files)} sample files...\")\n",
    "    \n",
    "    for cap_fp in tqdm(sample_files, desc=\"Sampling IATs\"):\n",
    "        try:\n",
    "            # Use count parameter to limit packet reading for large files\n",
    "            packets = rdpcap(cap_fp, count=10000)  # Limit to first 5000 packets\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {cap_fp}: {e}\", file=sys.stderr)\n",
    "            continue\n",
    "            \n",
    "        device_packets = extract_all_devices_packets(packets)\n",
    "        \n",
    "        for dev_name, mac in DEVICE_MACS.items():\n",
    "            time_pkt = device_packets.get(mac, [])\n",
    "            if len(time_pkt) < 2:\n",
    "                continue\n",
    "            times = np.array([t for t, _, _ in time_pkt])\n",
    "            iat = np.diff(times)\n",
    "            device_iats[dev_name].extend(iat.tolist())\n",
    "        \n",
    "        # Clear memory\n",
    "        del packets\n",
    "        gc.collect()\n",
    "    \n",
    "    device_stats = {}\n",
    "    for dev, iats in device_iats.items():\n",
    "        if not iats:\n",
    "            device_stats[dev] = {\"median\": 0.5, \"mean\": 0.5, \"std\": 0.1}\n",
    "        else:\n",
    "            iats = np.array(iats)\n",
    "            device_stats[dev] = {\n",
    "                \"median\": float(np.median(iats)),\n",
    "                \"mean\": float(np.mean(iats)),\n",
    "                \"std\": float(np.std(iats)),\n",
    "            }\n",
    "    return device_stats\n",
    "\n",
    "# ==========================================================\n",
    "# FIXED FEATURE ENGINEERING FUNCTION\n",
    "# ==========================================================\n",
    "def featurize_burst(burst, mac, filename, burst_id):\n",
    "    \"\"\"Fixed version with proper return statement\"\"\"\n",
    "    try:\n",
    "        import numpy as np\n",
    "        from math import log2\n",
    "\n",
    "        def entropy(values):\n",
    "            if len(values) == 0:\n",
    "                return 0.0\n",
    "            vals, counts = np.unique(values, return_counts=True)\n",
    "            probs = counts / counts.sum()\n",
    "            return -np.sum(probs * np.log2(probs + 1e-12))\n",
    "\n",
    "        def safe_div(a, b):\n",
    "            return a / (b + 1e-12)\n",
    "\n",
    "        def skewness(x):\n",
    "            x = np.asarray(x)\n",
    "            if x.size < 2:\n",
    "                return 0.0\n",
    "            m = x.mean()\n",
    "            s = x.std(ddof=0) + 1e-12\n",
    "            return np.mean((x - m) ** 3) / (s ** 3)\n",
    "\n",
    "        def kurtosis_excess(x):\n",
    "            x = np.asarray(x)\n",
    "            if x.size < 2:\n",
    "                return 0.0\n",
    "            m = x.mean()\n",
    "            s = x.std(ddof=0) + 1e-12\n",
    "            return np.mean((x - m) ** 4) / (s ** 4) - 3.0\n",
    "\n",
    "        def hurst_rs(ts):\n",
    "            ts = np.asarray(ts)\n",
    "            n = ts.size\n",
    "            if n < 10:\n",
    "                return 0.5\n",
    "            lags = np.floor(np.logspace(1, np.log10(n/2), num=10)).astype(int)\n",
    "            rs = []\n",
    "            for L in np.unique(lags):\n",
    "                segment = ts[:L]\n",
    "                if segment.size < 2:\n",
    "                    continue\n",
    "                mean_seg = segment.mean()\n",
    "                cum = np.cumsum(segment - mean_seg)\n",
    "                R = cum.max() - cum.min()\n",
    "                S = segment.std(ddof=0) + 1e-12\n",
    "                rs.append(R / S)\n",
    "            if len(rs) < 2:\n",
    "                return 0.5\n",
    "            x = np.log(np.arange(1, len(rs)+1))\n",
    "            y = np.log(rs)\n",
    "            slope = np.polyfit(x, y, 1)[0]\n",
    "            return max(0.0, min(1.0, slope))\n",
    "\n",
    "        times = np.array([t for t, _, _ in burst])\n",
    "        sizes = np.array([s for _, s, _ in burst])\n",
    "\n",
    "        inter_arrival = np.diff(times) if len(times) > 1 else np.array([0.0])\n",
    "        \n",
    "        # Calculate basic statistics once\n",
    "        total_pkts = len(burst)\n",
    "        total_bytes = np.sum(sizes)\n",
    "        flow_duration = times[-1] - times[0] if len(times) > 1 else 0.0\n",
    "        mean_pkt_size = float(np.mean(sizes)) if sizes.size else 0.0\n",
    "        std_pkt_size = float(np.std(sizes)) if sizes.size else 0.0\n",
    "        min_pkt_size = float(np.min(sizes)) if sizes.size else 0.0\n",
    "        max_pkt_size = float(np.max(sizes)) if sizes.size else 0.0\n",
    "        mean_iat = float(np.mean(inter_arrival))\n",
    "        std_iat = float(np.std(inter_arrival))\n",
    "        min_iat = float(np.min(inter_arrival))\n",
    "        max_iat = float(np.max(inter_arrival))\n",
    "        throughput = float(total_bytes / (flow_duration + 1e-6))\n",
    "\n",
    "        feat = {\n",
    "            \"file\": os.path.basename(filename),\n",
    "            \"device_mac\": mac,\n",
    "            \"burst_id\": burst_id,\n",
    "            \"packet_count\": total_pkts,\n",
    "            \"duration\": flow_duration,\n",
    "            \"mean_pkt_size\": mean_pkt_size,\n",
    "            \"std_pkt_size\": std_pkt_size,\n",
    "            \"min_pkt_size\": min_pkt_size,\n",
    "            \"max_pkt_size\": max_pkt_size,\n",
    "            \"mean_iat\": mean_iat,\n",
    "            \"std_iat\": std_iat,\n",
    "            \"min_iat\": min_iat,\n",
    "            \"max_iat\": max_iat,\n",
    "            \"throughput\": throughput,\n",
    "            \"burst_start_time\": times[0],\n",
    "            \"burst_end_time\": times[-1],\n",
    "        }\n",
    "\n",
    "        # Packet classification and counting\n",
    "        fwd_sizes, bwd_sizes = [], []\n",
    "        control_count = management_count = data_count = 0\n",
    "        null_count = probe_req_count = probe_resp_count = 0\n",
    "        ack_count = action_packet_count = block_ack_packet_count = 0\n",
    "        max_consec_qos_data = 0\n",
    "        current_qos_streak = 0\n",
    "        seq_numbers = []\n",
    "        frag_flag_count = 0\n",
    "        qos_count = 0\n",
    "        retry_flags = 0\n",
    "        to_ds = from_ds = 0\n",
    "\n",
    "        for t, s, pkt in burst:\n",
    "            dot11 = pkt[Dot11]\n",
    "            src, dst = dot11.addr2, dot11.addr1\n",
    "\n",
    "            if src == mac:\n",
    "                fwd_sizes.append(s)\n",
    "            elif dst == mac:\n",
    "                bwd_sizes.append(s)\n",
    "\n",
    "            pkt_type = getattr(dot11, \"type\", None)\n",
    "            pkt_subtype = getattr(dot11, \"subtype\", None)\n",
    "            fc = getattr(dot11, \"FCfield\", 0)\n",
    "            \n",
    "            if fc & 0x08:\n",
    "                retry_flags += 1\n",
    "                \n",
    "            frag = getattr(dot11, \"frag\", 0)\n",
    "            if frag != 0:\n",
    "                frag_flag_count += 1\n",
    "                \n",
    "            seq = getattr(dot11, \"SC\", None)\n",
    "            if seq is not None:\n",
    "                try:\n",
    "                    seqnum = (int(seq) >> 4) & 0xFFF\n",
    "                    seq_numbers.append(seqnum)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                    \n",
    "            if pkt_type == 2 and pkt_subtype in range(8,16):\n",
    "                qos_count += 1\n",
    "                \n",
    "            if isinstance(fc, int):\n",
    "                if fc & 0x01: to_ds += 1\n",
    "                if fc & 0x02: from_ds += 1\n",
    "\n",
    "            if pkt_type == 0:  # Management\n",
    "                management_count += 1\n",
    "                if pkt_subtype == 4:\n",
    "                    probe_req_count += 1\n",
    "                elif pkt_subtype == 5:\n",
    "                    probe_resp_count += 1\n",
    "                elif pkt_subtype == 13:\n",
    "                    action_packet_count += 1\n",
    "                current_qos_streak = 0\n",
    "            elif pkt_type == 1:  # Control\n",
    "                control_count += 1\n",
    "                if pkt_subtype == 13:\n",
    "                    ack_count += 1\n",
    "                elif pkt_subtype == 9:\n",
    "                    block_ack_packet_count += 1\n",
    "                current_qos_streak = 0\n",
    "            elif pkt_type == 2:  # Data\n",
    "                data_count += 1\n",
    "                if pkt_subtype in range(8,16):\n",
    "                    current_qos_streak += 1\n",
    "                    max_consec_qos_data = max(max_consec_qos_data, current_qos_streak)\n",
    "                else:\n",
    "                    current_qos_streak = 0\n",
    "                if pkt_subtype in [0x4,0xC]:\n",
    "                    null_count += 1\n",
    "\n",
    "        # Calculate derived features\n",
    "        uplink_packet_ratio = len(fwd_sizes) / (total_pkts + 1e-6)\n",
    "        downlink_packet_ratio = len(bwd_sizes) / (total_pkts + 1e-6)\n",
    "        ack_to_data_ratio = safe_div(control_count, data_count)\n",
    "        entropy_pkt_size = entropy(sizes)\n",
    "        burstiness_index = safe_div((max_iat - mean_iat), mean_iat) if inter_arrival.size > 1 else 0.0\n",
    "        \n",
    "        unique_peer_count = len(set([pkt[Dot11].addr1 for _, _, pkt in burst] + [pkt[Dot11].addr2 for _, _, pkt in burst]))\n",
    "        payload_ratio = safe_div(total_bytes, (total_pkts * max_pkt_size))\n",
    "        seq_entropy = entropy(seq_numbers) if seq_numbers else 0.0\n",
    "        tods_ratio = safe_div(to_ds, total_pkts)\n",
    "        fromds_ratio = safe_div(from_ds, total_pkts)\n",
    "\n",
    "        # Burst behavior features\n",
    "        median_iat = float(np.median(inter_arrival)) if inter_arrival.size else 0.0\n",
    "        active_windows = np.sum(inter_arrival < (median_iat + 1e-12)) if inter_arrival.size else 0\n",
    "        active_time_ratio = safe_div(active_windows, len(inter_arrival))\n",
    "        \n",
    "        mid_idx = total_pkts // 2\n",
    "        first_half_sizes = sizes[:mid_idx] if total_pkts>1 else sizes\n",
    "        second_half_sizes = sizes[mid_idx:] if total_pkts>1 else sizes\n",
    "        first_sum = float(np.sum(first_half_sizes)) if first_half_sizes.size else 0.0\n",
    "        second_sum = float(np.sum(second_half_sizes)) if second_half_sizes.size else 0.0\n",
    "        burst_symmetry = safe_div(abs(first_sum-second_sum), (first_sum+second_sum))\n",
    "\n",
    "        # Peak density calculation\n",
    "        if flow_duration <= 0:\n",
    "            peak_packet_density = total_pkts\n",
    "        else:\n",
    "            window = 1.0\n",
    "            i = 0\n",
    "            max_density = 0\n",
    "            N = len(times)\n",
    "            for start_idx in range(N):\n",
    "                end_time = times[start_idx] + window\n",
    "                while i < N and times[i] <= end_time:\n",
    "                    i += 1\n",
    "                count_window = i - start_idx\n",
    "                if count_window > max_density:\n",
    "                    max_density = count_window\n",
    "            peak_packet_density = max_density\n",
    "\n",
    "        # Packet arrival slope\n",
    "        if flow_duration>0 and total_pkts>1:\n",
    "            t_rel = times - times[0]\n",
    "            cum_counts = np.arange(1,total_pkts+1)\n",
    "            denom = np.var(t_rel) + 1e-12\n",
    "            slope = float(np.cov(t_rel,cum_counts,bias=True)[0,1]/denom)\n",
    "        else:\n",
    "            slope = safe_div(total_pkts-1, flow_duration)\n",
    "\n",
    "        # Statistical features\n",
    "        iat_cv = safe_div(std_iat, mean_iat)\n",
    "        iat_skew = float(skewness(inter_arrival))\n",
    "        iat_kurt = float(kurtosis_excess(inter_arrival))\n",
    "        iat_entropy = float(entropy(np.round(inter_arrival,6)))\n",
    "        size_skew = float(skewness(sizes))\n",
    "        size_kurt = float(kurtosis_excess(sizes))\n",
    "        size_entropy = float(entropy(np.round(sizes,0)))\n",
    "\n",
    "        # Hurst exponent\n",
    "        try: \n",
    "            hurst = float(hurst_rs(inter_arrival))\n",
    "        except Exception: \n",
    "            hurst=0.5\n",
    "\n",
    "        # IAT histogram\n",
    "        if inter_arrival.size:\n",
    "            hist_counts,_ = np.histogram(inter_arrival,bins=5)\n",
    "            iat_hist = (hist_counts/(hist_counts.sum()+1e-12)).tolist()\n",
    "        else:\n",
    "            iat_hist = [0.0]*5\n",
    "\n",
    "        # Additional requested features\n",
    "        InitBwdWinByts = float(sum(bwd_sizes[:5])) if len(bwd_sizes) else 0.0\n",
    "        BwdPktLenMax = float(np.max(bwd_sizes)) if len(bwd_sizes) else 0.0\n",
    "        FwdPktLenMax = float(np.max(fwd_sizes)) if len(fwd_sizes) else 0.0\n",
    "        BwdPktsPerSec = safe_div(len(bwd_sizes), flow_duration)\n",
    "\n",
    "        # Assemble final feature dictionary\n",
    "        feat.update({\n",
    "            \"uplink_packet_ratio\": float(uplink_packet_ratio),\n",
    "            \"downlink_packet_ratio\": float(downlink_packet_ratio),\n",
    "            \"retry_count\": int(retry_flags),\n",
    "            \"ack_to_data_ratio\": float(ack_to_data_ratio),\n",
    "            \"entropy_pkt_size\": float(entropy_pkt_size),\n",
    "            \"burstiness_index\": float(burstiness_index),\n",
    "            \"unique_peer_count\": int(unique_peer_count),\n",
    "            \"payload_ratio\": float(payload_ratio),\n",
    "            \"control_packet_count\": int(control_count),\n",
    "            \"management_packet_count\": int(management_count),\n",
    "            \"data_packet_count\": int(data_count),\n",
    "            \"null_packet_count\": int(null_count),\n",
    "            \"probe_request_count\": int(probe_req_count),\n",
    "            \"probe_response_count\": int(probe_resp_count),\n",
    "            \"ack_packet_count\": int(ack_count),\n",
    "            \"action_packet_count\": int(action_packet_count),\n",
    "            \"block_ack_packet_count\": int(block_ack_packet_count),\n",
    "            \"max_consec_qos_data_count\": int(max_consec_qos_data),\n",
    "            \"seq_entropy\": float(seq_entropy),\n",
    "            \"frag_flag_count\": int(frag_flag_count),\n",
    "            \"qos_packet_count\": int(qos_count),\n",
    "            \"tods_ratio\": float(tods_ratio),\n",
    "            \"fromds_ratio\": float(fromds_ratio),\n",
    "            \"retry_flags_count\": int(retry_flags),\n",
    "            \"active_time_ratio\": float(active_time_ratio),\n",
    "            \"burst_symmetry\": float(burst_symmetry),\n",
    "            \"peak_packet_density_1s\": int(peak_packet_density),\n",
    "            \"packet_arrival_slope\": float(slope),\n",
    "            \"iat_cv\": float(iat_cv),\n",
    "            \"iat_skew\": float(iat_skew),\n",
    "            \"iat_kurtosis\": float(iat_kurt),\n",
    "            \"iat_entropy\": float(iat_entropy),\n",
    "            \"size_skew\": float(size_skew),\n",
    "            \"size_kurtosis\": float(size_kurt),\n",
    "            \"size_entropy\": float(size_entropy),\n",
    "            \"hurst_exponent\": float(hurst),\n",
    "            \"iat_hist_bin0\": float(iat_hist[0]),\n",
    "            \"iat_hist_bin1\": float(iat_hist[1]),\n",
    "            \"iat_hist_bin2\": float(iat_hist[2]),\n",
    "            \"iat_hist_bin3\": float(iat_hist[3]),\n",
    "            \"iat_hist_bin4\": float(iat_hist[4]),\n",
    "            \"InitBwdWinByts\": InitBwdWinByts,\n",
    "            \"FlowIATMin\": min_iat,\n",
    "            \"FlowIATMax\": max_iat,\n",
    "            \"BwdPktLenMax\": BwdPktLenMax,\n",
    "            \"FlowDuration\": flow_duration,\n",
    "            \"FlowBytesPerSec\": throughput,\n",
    "            \"BwdPktsPerSec\": BwdPktsPerSec,\n",
    "            \"FwdPktLenMax\": FwdPktLenMax,\n",
    "            \"PktLenMin\": min_pkt_size,\n",
    "            \"PktLenMax\": max_pkt_size,\n",
    "        })\n",
    "\n",
    "        return feat  # MAKE SURE THIS RETURN STATEMENT EXISTS\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in featurize_burst for {filename} burst {burst_id}: {e}\")\n",
    "        return None  # Return None instead of crashing\n",
    "\n",
    "def process_single_cap_optimized(args):\n",
    "    \"\"\"Process a single capture file for all devices with proper error handling\"\"\"\n",
    "    cap_fp, thresholds = args\n",
    "    try:\n",
    "        # Use count to limit very large files\n",
    "        packets = rdpcap(cap_fp, count=10000)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {cap_fp}: {e}\", file=sys.stderr)\n",
    "        return []\n",
    "    \n",
    "    feats = []\n",
    "    device_packets = extract_all_devices_packets(packets)\n",
    "    \n",
    "    for dev_name, mac in DEVICE_MACS.items():\n",
    "        time_pkt = device_packets.get(mac, [])\n",
    "        if not time_pkt:\n",
    "            continue\n",
    "            \n",
    "        gap_threshold = thresholds.get(dev_name, 0.5)\n",
    "        bursts = segment_into_bursts(time_pkt, gap_threshold=gap_threshold)\n",
    "        \n",
    "        for i, burst in enumerate(bursts):\n",
    "            # Skip very small bursts\n",
    "            if len(burst) < 3:\n",
    "                continue\n",
    "                \n",
    "            feat = featurize_burst(burst, mac, cap_fp, i)\n",
    "            if feat is not None:  # Only add if featurization succeeded\n",
    "                feat[\"device_name\"] = dev_name\n",
    "                feat[\"adaptive_gap\"] = gap_threshold\n",
    "                feats.append(feat)\n",
    "    \n",
    "    # Clear memory\n",
    "    del packets\n",
    "    gc.collect()\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def process_all_captures_optimized(root_dir, thresholds, max_workers=None):\n",
    "    \"\"\"Process all captures with optimized parallel processing\"\"\"\n",
    "    cap_files = find_all_cap_files(root_dir)\n",
    "    \n",
    "    if max_workers is None:\n",
    "        max_workers = min(cpu_count(), 8)  # Limit workers to avoid memory issues\n",
    "    \n",
    "    print(f\"Processing {len(cap_files)} files with {max_workers} workers...\")\n",
    "    \n",
    "    # Process in chunks to manage memory\n",
    "    chunk_size = min(100, len(cap_files) // (max_workers * 2) + 1)\n",
    "    \n",
    "    all_feats = []\n",
    "    for i in range(0, len(cap_files), chunk_size):\n",
    "        chunk_files = cap_files[i:i + chunk_size]\n",
    "        \n",
    "        with Pool(max_workers) as pool:\n",
    "            chunk_args = [(fp, thresholds) for fp in chunk_files]\n",
    "            results = list(tqdm(\n",
    "                pool.imap(process_single_cap_optimized, chunk_args),\n",
    "                total=len(chunk_files),\n",
    "                desc=f\"Processing chunk {i//chunk_size + 1}/{(len(cap_files)-1)//chunk_size + 1}\"\n",
    "            ))\n",
    "        \n",
    "        chunk_feats = [feat for sublist in results for feat in sublist]\n",
    "        all_feats.extend(chunk_feats)\n",
    "        \n",
    "        # Clear memory between chunks\n",
    "        gc.collect()\n",
    "    \n",
    "    df = pd.DataFrame(all_feats)\n",
    "    return df\n",
    "\n",
    "def segment_into_bursts(time_pkt, gap_threshold=0.1):\n",
    "    if not time_pkt:\n",
    "        return []\n",
    "    bursts, current = [], [time_pkt[0]]\n",
    "    for i in range(1, len(time_pkt)):\n",
    "        if time_pkt[i][0] - time_pkt[i - 1][0] > gap_threshold:\n",
    "            bursts.append(current)\n",
    "            current = [time_pkt[i]]\n",
    "        else:\n",
    "            current.append(time_pkt[i])\n",
    "    bursts.append(current)\n",
    "    return bursts\n",
    "\n",
    "def compute_adaptive_thresholds(device_stats, factor_dict=None):\n",
    "    thresholds = {}\n",
    "    for dev, stats in device_stats.items():\n",
    "        factor = 2.0\n",
    "        if factor_dict and dev in factor_dict:\n",
    "            factor = factor_dict[dev]\n",
    "        thresholds[dev] = max(0.001, stats[\"median\"] * factor)\n",
    "    return thresholds\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN EXECUTION\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Collecting IAT stats per device (optimized)...\")\n",
    "    device_stats = collect_device_iat_stats_optimized(CAPTURE_ROOT)\n",
    "    \n",
    "    print(\"Computing adaptive thresholds per device...\")\n",
    "    adaptive_thresholds = compute_adaptive_thresholds(device_stats)\n",
    "    \n",
    "    print(\"Processing captures with adaptive thresholds...\")\n",
    "    df = process_all_captures_optimized(CAPTURE_ROOT, adaptive_thresholds)\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(f\"Saved {len(df)} bursts to {OUTPUT_CSV}\")\n",
    "    else:\n",
    "        print(\"No features extracted - output CSV not created\")\n",
    "    \n",
    "    print(\"Adaptive thresholds used per device:\")\n",
    "    for dev, t in adaptive_thresholds.items():\n",
    "        print(f\" - {dev}: {t:.4f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290a464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot44",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
